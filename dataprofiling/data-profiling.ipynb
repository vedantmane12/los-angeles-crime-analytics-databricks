{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe2c529-bb31-453a-a3f1-f11616e89c39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Los Angeles Crime Analytics: Data Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5634de1-d012-40c2-85dc-6574bb7c322a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d5e558-e46e-4e60-abfe-1a1b98387428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Part 1: Environment Setup & Configuration\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b24fd90f-775c-464f-8d7c-ab923f274c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Download Data from LA Open Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63112b23-0e91-4bc9-84ac-07e19477a980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Download data and save to volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4793bc3f-da4c-407d-81b0-cc9ad54f1f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "catalog = \"workspace\"\n",
    "schema = \"la_crime_schema\"\n",
    "volume = \"datastore\"\n",
    "download_url = \"https://data.lacity.org/resource/2nrs-mtv8.csv?$limit=10000000\"\n",
    "file_name = \"crime_data_raw.csv\"\n",
    "\n",
    "# Paths\n",
    "path_volume = f\"/Volumes/{catalog}/{schema}/{volume}\"\n",
    "file_path = f\"{path_volume}/{file_name}\"\n",
    "\n",
    "# Download data using pandas\n",
    "try:\n",
    "    df_pandas = pd.read_csv(download_url)    \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading data: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Save to Unity Catalog Volume\n",
    "try:\n",
    "    df_pandas.to_csv(file_path, index=False)    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d42656a-67f2-4f5f-ac2a-6102aaf0192a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load into Spark DataFrame for profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24209e46-76b6-4cb9-8481-a1cd91456c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load into Spark DataFrame for profiling\n",
    "try:\n",
    "    df_raw = spark.read.csv(\n",
    "        file_path,\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "        mode=\"PERMISSIVE\"\n",
    "    )\n",
    "    # Get basic stats\n",
    "    row_count = df_raw.count()\n",
    "    col_count = len(df_raw.columns)\n",
    "    print(f\"Data loaded successfully!\")\n",
    "    print(f\"Total Records: {row_count:,}\")\n",
    "    print(f\"Total Columns: {col_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Preview of the data\n",
    "display(df_raw.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1068e666-0b8e-4af3-94fb-0a53cece32d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Understanding the Source Schema & Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d351b0dd-e3fb-48fb-9ad2-8758c18560c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a detailed schema summary table\n",
    "\n",
    "schema_info = []\n",
    "for field in df_raw.schema.fields:\n",
    "    schema_info.append({\n",
    "        'Column_Name': field.name,\n",
    "        'Data_Type': str(field.dataType),\n",
    "        'Nullable': field.nullable,\n",
    "        'Category': 'TBD'  # We'll categorize later\n",
    "    })\n",
    "\n",
    "schema_df = spark.createDataFrame(schema_info)\n",
    "\n",
    "# Get basic statistics\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(f\"Total Records: {df_raw.count():,}\")\n",
    "print(f\"Total Columns: {len(df_raw.columns)}\")\n",
    "print(f\"Storage Location: {file_path}\")\n",
    "\n",
    "# Column names list\n",
    "print(\"Columns\")\n",
    "for i, col in enumerate(df_raw.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39418991-366b-492c-b07e-9a9604b00723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Null Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004260bf-1080-4bc2-ab36-4a577202c606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate null counts and percentages for all columns\n",
    "null_analysis = []\n",
    "total_records = df_raw.count()\n",
    "\n",
    "for col_name in df_raw.columns:\n",
    "    null_count = df_raw.filter(F.col(col_name).isNull()).count()\n",
    "    null_percentage = (null_count / total_records) * 100\n",
    "    non_null_count = total_records - null_count\n",
    "    \n",
    "    null_analysis.append({\n",
    "        'Column': col_name,\n",
    "        'Null_Count': null_count,\n",
    "        'Null_Percentage': round(null_percentage, 2),\n",
    "        'Non_Null_Count': non_null_count,\n",
    "        'Completeness': f\"{100 - null_percentage:.2f}%\"\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort by null percentage (highest first)\n",
    "null_df = spark.createDataFrame(null_analysis).orderBy(F.desc('Null_Percentage'))\n",
    "\n",
    "# Display full null analysis results\n",
    "print(\"NULL Analysis Results\")\n",
    "display(null_df)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"NULL Analysis Summary\")\n",
    "\n",
    "total_cells = total_records * len(df_raw.columns)\n",
    "total_null_cells = sum([row['Null_Count'] for row in null_analysis])\n",
    "completeness_percentage = ((total_cells - total_null_cells) / total_cells) * 100\n",
    "\n",
    "print(f\"Total Records: {total_records:,}\")\n",
    "print(f\"Total Columns: {len(df_raw.columns)}\")\n",
    "print(f\"Total Cells: {total_cells:,}\")\n",
    "print(f\"Null Cells: {total_null_cells:,}\")\n",
    "print(f\"Non-Null Cells: {total_cells - total_null_cells:,}\")\n",
    "print(f\"\\nOverall Data Completeness: {completeness_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ed98d1-d24d-4494-b2c0-7d04e61b9041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Cardinality Analysis (Distinct Value Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ce1631-9f5f-4308-bfed-a39dba2e9d0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cardinality Analysis (Distinct Value Counts)\n",
    "\n",
    "# Calculate distinct counts for all columns\n",
    "cardinality_analysis = []\n",
    "total_records = df_raw.count()\n",
    "\n",
    "for col_name in df_raw.columns:\n",
    "    distinct_count = df_raw.select(col_name).distinct().count()\n",
    "    cardinality_ratio = (distinct_count / total_records) * 100\n",
    "    # Categorize cardinality\n",
    "    if cardinality_ratio > 90:\n",
    "        category = \"Very High (Identifier)\"\n",
    "    elif cardinality_ratio > 50:\n",
    "        category = \"High\"\n",
    "    elif cardinality_ratio > 10:\n",
    "        category = \"Medium-High\"\n",
    "    elif cardinality_ratio > 1:\n",
    "        category = \"Medium\"\n",
    "    else:\n",
    "        category = \"Low (Good Dimension)\"\n",
    "    cardinality_analysis.append({\n",
    "        'Column': col_name,\n",
    "        'Distinct_Count': distinct_count,\n",
    "        'Total_Records': total_records,\n",
    "        'Cardinality_Ratio_%': round(cardinality_ratio, 2),\n",
    "        'Category': category\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort by distinct count\n",
    "cardinality_df = spark.createDataFrame(cardinality_analysis).orderBy(F.desc('Distinct_Count'))\n",
    "\n",
    "# Display full cardinality results\n",
    "print(\"Cardinality Analysis\")\n",
    "display(cardinality_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935fb9f9-5e2b-4a83-bc3a-b3a80a2e307c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Temporal Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6017025-1502-4148-bdbc-93cc4693c39a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Temporal Data Validation\n",
    "\n",
    "# Analyze date_occ (occurrence date)\n",
    "date_occ_stats = df_raw.select(\n",
    "    F.min('date_occ').alias('Min_Date_Occ'),\n",
    "    F.max('date_occ').alias('Max_Date_Occ'),\n",
    "    F.count('date_occ').alias('Total_Records')\n",
    ").collect()[0]\n",
    "\n",
    "print(\"Date Occurred (date_occ):\")\n",
    "print(f\"  Earliest: {date_occ_stats['Min_Date_Occ']}\")\n",
    "print(f\"  Latest: {date_occ_stats['Max_Date_Occ']}\")\n",
    "print(f\"  Non-Null Records: {date_occ_stats['Total_Records']:,}\")\n",
    "\n",
    "# Analyze date_rptd (reported date)\n",
    "date_rptd_stats = df_raw.select(\n",
    "    F.min('date_rptd').alias('Min_Date_Rptd'),\n",
    "    F.max('date_rptd').alias('Max_Date_Rptd'),\n",
    "    F.count('date_rptd').alias('Total_Records')\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nDate Reported (date_rptd):\")\n",
    "print(f\"  Earliest: {date_rptd_stats['Min_Date_Rptd']}\")\n",
    "print(f\"  Latest: {date_rptd_stats['Max_Date_Rptd']}\")\n",
    "print(f\"  Non-Null Records: {date_rptd_stats['Total_Records']:,}\")\n",
    "\n",
    "# Check for date logic issues: date_rptd should be >= date_occ\n",
    "print(\"Date Validation: Report date should be >= Occurrence date\")\n",
    "\n",
    "# Count records where report date is BEFORE occurrence date (illogical)\n",
    "illogical_dates = df_raw.filter(F.col('date_rptd') < F.col('date_occ')).count()\n",
    "\n",
    "print(f\"Records where date_rptd < date_occ: {illogical_dates:,}\")\n",
    "\n",
    "if illogical_dates > 0:\n",
    "    illogical_pct = (illogical_dates / total_records) * 100\n",
    "    print(f\"Percentage: {illogical_pct:.2f}%\")\n",
    "    print(\"\\nSample illogical records:\")\n",
    "    display(df_raw.filter(F.col('date_rptd') < F.col('date_occ'))\n",
    "            .select('dr_no', 'date_rptd', 'date_occ', 'crm_cd_desc')\n",
    "            .limit(10))\n",
    "else:\n",
    "    print(\"All dates follow logical order!\")\n",
    "\n",
    "print(\"Reporting LAG Anaysis\")\n",
    "\n",
    "df_with_lag = df_raw.withColumn(\n",
    "    'days_to_report',\n",
    "    F.datediff(F.col('date_rptd'), F.col('date_occ'))\n",
    ")\n",
    "\n",
    "lag_stats = df_with_lag.select(\n",
    "    F.min('days_to_report').alias('Min_Days'),\n",
    "    F.max('days_to_report').alias('Max_Days'),\n",
    "    F.avg('days_to_report').alias('Avg_Days'),\n",
    "    F.expr('percentile(days_to_report, 0.5)').alias('Median_Days'),\n",
    "    F.expr('percentile(days_to_report, 0.90)').alias('P90_Days')\n",
    ").collect()[0]\n",
    "\n",
    "print(\"Days between occurrence and report:\")\n",
    "print(f\"  Minimum: {lag_stats['Min_Days']} days\")\n",
    "print(f\"  Maximum: {lag_stats['Max_Days']} days\")\n",
    "print(f\"  Average: {lag_stats['Avg_Days']:.2f} days\")\n",
    "print(f\"  Median: {lag_stats['Median_Days']} days\")\n",
    "print(f\"  90th Percentile: {lag_stats['P90_Days']} days\")\n",
    "\n",
    "# Distribution of reporting lag\n",
    "print(\"\\nReporting Lag Distribution:\")\n",
    "lag_distribution = df_with_lag.groupBy('days_to_report').count().orderBy('days_to_report').limit(20)\n",
    "display(lag_distribution)\n",
    "\n",
    "# TIME_OCC validation (should be 0000-2359 in 24-hour format)\n",
    "print(\"Time Field Validation\")\n",
    "\n",
    "# Check for invalid time values\n",
    "invalid_times = df_raw.filter(\n",
    "    (F.col('time_occ') < 0) | (F.col('time_occ') > 2359)\n",
    ").count()\n",
    "\n",
    "print(f\"Invalid time_occ values (outside 0-2359): {invalid_times:,}\")\n",
    "\n",
    "# Check for times with invalid minutes (>59)\n",
    "# Extract hour and minute\n",
    "df_time_check = df_raw.withColumn('time_str', F.lpad(F.col('time_occ').cast('string'), 4, '0'))\n",
    "df_time_check = df_time_check.withColumn('hour', F.substring('time_str', 1, 2).cast('int'))\n",
    "df_time_check = df_time_check.withColumn('minute', F.substring('time_str', 3, 2).cast('int'))\n",
    "\n",
    "invalid_hours = df_time_check.filter((F.col('hour') > 23) | (F.col('hour') < 0)).count()\n",
    "invalid_minutes = df_time_check.filter((F.col('minute') > 59) | (F.col('minute') < 0)).count()\n",
    "\n",
    "print(f\"Invalid hours (>23 or <0): {invalid_hours:,}\")\n",
    "print(f\"Invalid minutes (>59 or <0): {invalid_minutes:,}\")\n",
    "\n",
    "# Show time distribution by hour\n",
    "print(\"\\nCrime Distribution by Hour of Day:\")\n",
    "hour_distribution = df_time_check.groupBy('hour').count().orderBy('hour')\n",
    "display(hour_distribution)\n",
    "\n",
    "# Check for future dates (data quality issue)\n",
    "print(\"Date Checks\")\n",
    "\n",
    "current_date = datetime.now()\n",
    "\n",
    "future_occ = df_raw.filter(F.col('date_occ') > F.lit(current_date)).count()\n",
    "future_rptd = df_raw.filter(F.col('date_rptd') > F.lit(current_date)).count()\n",
    "\n",
    "print(f\"Occurrences in the future: {future_occ:,}\")\n",
    "print(f\"Reports in the future: {future_rptd:,}\")\n",
    "\n",
    "if future_occ > 0 or future_rptd > 0:\n",
    "    print(\"WARNING: Found future dates!\")\n",
    "    if future_occ > 0:\n",
    "        print(\"\\nSample future occurrences:\")\n",
    "        display(df_raw.filter(F.col('date_occ') > F.lit(current_date))\n",
    "                .select('dr_no', 'date_occ', 'date_rptd', 'crm_cd_desc')\n",
    "                .limit(10))\n",
    "else:\n",
    "    print(\"No future dates found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a543483-c18f-422c-83bd-a74607c4d09e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Geographic Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3551474b-1702-4c36-a5f9-2ca7444af5ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Coordinate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee363fa0-ba08-4597-a66e-a64c1e85b882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Coordinate statistics\n",
    "coord_stats = df_raw.select(\n",
    "    F.min('lat').alias('Min_LAT'),\n",
    "    F.max('lat').alias('Max_LAT'),\n",
    "    F.avg('lat').alias('Avg_LAT'),\n",
    "    F.min('lon').alias('Min_LON'),\n",
    "    F.max('lon').alias('Max_LON'),\n",
    "    F.avg('lon').alias('Avg_LON'),\n",
    "    F.count('lat').alias('Non_Null_LAT'),\n",
    "    F.count('lon').alias('Non_Null_LON')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nLatitude Statistics:\")\n",
    "print(f\"  Min: {coord_stats['Min_LAT']:.6f}\")\n",
    "print(f\"  Max: {coord_stats['Max_LAT']:.6f}\")\n",
    "print(f\"  Avg: {coord_stats['Avg_LAT']:.6f}\")\n",
    "print(f\"  Non-Null: {coord_stats['Non_Null_LAT']:,}\")\n",
    "\n",
    "print(f\"\\nLongitude Statistics:\")\n",
    "print(f\"  Min: {coord_stats['Min_LON']:.6f}\")\n",
    "print(f\"  Max: {coord_stats['Max_LON']:.6f}\")\n",
    "print(f\"  Avg: {coord_stats['Avg_LON']:.6f}\")\n",
    "print(f\"  Non-Null: {coord_stats['Non_Null_LON']:,}\")\n",
    "\n",
    "# Check for coordinates outside LA County bounds\n",
    "print(\"OUT-OF-BOUNDS Coordinate Analysis\")\n",
    "\n",
    "# Zero coordinates (0, 0) - data quality issue\n",
    "zero_coords = df_raw.filter((F.col('lat') == 0) & (F.col('lon') == 0)).count()\n",
    "print(f\"Zero coordinates (0, 0): {zero_coords:,}\")\n",
    "\n",
    "# Out of LA County bounds\n",
    "out_of_bounds_lat = df_raw.filter(\n",
    "    (F.col('lat') < 33.7) | (F.col('lat') > 34.3)\n",
    ").count()\n",
    "\n",
    "out_of_bounds_lon = df_raw.filter(\n",
    "    (F.col('lon') < -118.7) | (F.col('lon') > -118.1)\n",
    ").count()\n",
    "\n",
    "out_of_bounds_both = df_raw.filter(\n",
    "    ((F.col('lat') < 33.7) | (F.col('lat') > 34.3)) |\n",
    "    ((F.col('lon') < -118.7) | (F.col('lon') > -118.1))\n",
    ").count()\n",
    "\n",
    "print(f\"Out-of-bounds Latitude: {out_of_bounds_lat:,}\")\n",
    "print(f\"Out-of-bounds Longitude: {out_of_bounds_lon:,}\")\n",
    "print(f\"Out-of-bounds (either): {out_of_bounds_both:,}\")\n",
    "\n",
    "if out_of_bounds_both > 0:\n",
    "    out_of_bounds_pct = (out_of_bounds_both / total_records) * 100\n",
    "    print(f\"Percentage: {out_of_bounds_pct:.2f}%\")\n",
    "    print(\"WARNING: Found coordinates outside LA County!\")\n",
    "    \n",
    "    print(\"\\nSample out-of-bounds records:\")\n",
    "    display(df_raw.filter(\n",
    "        ((F.col('lat') < 33.7) | (F.col('lat') > 34.3)) |\n",
    "        ((F.col('lon') < -118.7) | (F.col('lon') > -118.1))\n",
    "    ).select('dr_no', 'lat', 'lon', 'location', 'area_name').limit(10))\n",
    "else:\n",
    "    print(\"All coordinates within LA County bounds!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85cb09f6-4e9f-4850-9e53-404a6ca381e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### LA Area Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9e1ee7c-c360-4327-94ae-5fef28734f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Area distribution (LAPD has 21 geographic areas)\n",
    "print(\"LAPD Area Distribution\")\n",
    "area_distribution = df_raw.groupBy('area', 'area_name').count().orderBy('area')\n",
    "\n",
    "print(\"\\nCrime count by Area:\")\n",
    "display(area_distribution)\n",
    "\n",
    "# Check for invalid area codes\n",
    "valid_areas = [str(i) for i in range(1, 22)]\n",
    "invalid_area_count = df_raw.filter(~F.col('area').cast('string').isin(valid_areas)).count()\n",
    "\n",
    "print(f\"\\nInvalid area codes (not 1-21): {invalid_area_count:,}\")\n",
    "\n",
    "if invalid_area_count > 0:\n",
    "    print(\"WARNING: Found invalid area codes!\")\n",
    "    display(df_raw.filter(~F.col('area').cast('string').isin(valid_areas))\n",
    "            .select('area', 'area_name').distinct())\n",
    "else:\n",
    "    print(\"All area codes valid (1-21)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aa411e4-cdf3-4410-858d-15d8f5c9b6f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### LA Reporting District analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "578ec043-cfe4-4746-a16a-c12276b8726a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reporting District analysis\n",
    "print(\"Reporting District Analysis\")\n",
    "\n",
    "rpt_dist_stats = df_raw.select(\n",
    "    F.countDistinct('rpt_dist_no').alias('Distinct_Districts'),\n",
    "    F.min('rpt_dist_no').alias('Min_District'),\n",
    "    F.max('rpt_dist_no').alias('Max_District')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Total Reporting Districts: {rpt_dist_stats['Distinct_Districts']:,}\")\n",
    "print(f\"District Range: {rpt_dist_stats['Min_District']} to {rpt_dist_stats['Max_District']}\")\n",
    "\n",
    "# Top 10 districts by crime count\n",
    "print(\"\\nTop 10 Reporting Districts by Crime Count:\")\n",
    "top_districts = df_raw.groupBy('rpt_dist_no').count().orderBy(F.desc('count')).limit(10)\n",
    "display(top_districts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "161c1e11-eada-405d-a89f-4f35bac18bcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Location field analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e2d21f4-16d4-4579-8104-defadcfe64ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Location field analysis\n",
    "print(\"# Location Field Analysis\")\n",
    "\n",
    "location_stats = df_raw.select(\n",
    "    F.count('location').alias('Non_Null_Locations'),\n",
    "    F.countDistinct('location').alias('Unique_Locations')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Non-Null Locations: {location_stats['Non_Null_Locations']:,}\")\n",
    "print(f\"Unique Locations: {location_stats['Unique_Locations']:,}\")\n",
    "\n",
    "# Sample locations\n",
    "print(\"\\nSample Location Values:\")\n",
    "display(df_raw.select('location', 'cross_street', 'area_name')\n",
    "        .filter(F.col('location').isNotNull())\n",
    "        .limit(10))\n",
    "\n",
    "# Location string length distribution\n",
    "location_length_stats = df_raw.filter(F.col('location').isNotNull()).select(\n",
    "    F.min(F.length('location')).alias('Min_Length'),\n",
    "    F.max(F.length('location')).alias('Max_Length'),\n",
    "    F.avg(F.length('location')).alias('Avg_Length')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nLocation String Length:\")\n",
    "print(f\"  Min: {location_length_stats['Min_Length']}\")\n",
    "print(f\"  Max: {location_length_stats['Max_Length']}\")\n",
    "print(f\"  Avg: {location_length_stats['Avg_Length']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b13af5f7-daf2-4b0e-9dfb-3d7da78e2d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8: Demographic Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "382241a3-2f9e-40da-bb2f-13a92e3dfcf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Victim Age Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783c96e8-cea2-48db-8b54-395335e94e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 8: Demographic Data Validation\n",
    "print(\"Age Analysis\")\n",
    "\n",
    "# Age statistics\n",
    "age_stats = df_raw.select(\n",
    "    F.min('vict_age').alias('Min_Age'),\n",
    "    F.max('vict_age').alias('Max_Age'),\n",
    "    F.avg('vict_age').alias('Avg_Age'),\n",
    "    F.expr('percentile(vict_age, 0.5)').alias('Median_Age'),\n",
    "    F.count('vict_age').alias('Non_Null_Count')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Age Statistics:\")\n",
    "print(f\"  Min: {age_stats['Min_Age']}\")\n",
    "print(f\"  Max: {age_stats['Max_Age']}\")\n",
    "print(f\"  Avg: {age_stats['Avg_Age']:.2f}\")\n",
    "print(f\"  Median: {age_stats['Median_Age']}\")\n",
    "print(f\"  Non-Null: {age_stats['Non_Null_Count']:,}\")\n",
    "\n",
    "# Check for invalid ages\n",
    "negative_ages = df_raw.filter(F.col('vict_age') < 0).count()\n",
    "zero_ages = df_raw.filter(F.col('vict_age') == 0).count()\n",
    "over_120 = df_raw.filter(F.col('vict_age') > 120).count()\n",
    "invalid_ages = df_raw.filter((F.col('vict_age') < 0) | (F.col('vict_age') > 120)).count()\n",
    "\n",
    "print(f\"\\nData Quality Issues:\")\n",
    "print(f\"  Negative ages: {negative_ages:,}\")\n",
    "print(f\"  Zero ages: {zero_ages:,}\")\n",
    "print(f\"  Ages > 120: {over_120:,}\")\n",
    "print(f\"  Total invalid: {invalid_ages:,} ({(invalid_ages/total_records)*100:.2f}%)\")\n",
    "\n",
    "if invalid_ages > 0:\n",
    "    print(\"\\nWARNING: Found invalid ages!\")\n",
    "    print(\"Sample invalid ages:\")\n",
    "    display(df_raw.filter((F.col('vict_age') < 0) | (F.col('vict_age') > 120))\n",
    "            .select('dr_no', 'vict_age', 'vict_sex', 'vict_descent', 'crm_cd_desc')\n",
    "            .limit(10))\n",
    "else:\n",
    "    print(\"\\nAll ages are valid (0-120)\")\n",
    "\n",
    "# Age distribution\n",
    "print(\"Age Distribution\")\n",
    "\n",
    "# Create age groups for analysis\n",
    "df_age_groups = df_raw.withColumn(\n",
    "    'age_group',\n",
    "    F.when(F.col('vict_age') == 0, 'Unknown')\n",
    "    .when(F.col('vict_age') < 18, '0-17 (Juvenile)')\n",
    "    .when(F.col('vict_age') < 25, '18-24')\n",
    "    .when(F.col('vict_age') < 35, '25-34')\n",
    "    .when(F.col('vict_age') < 45, '35-44')\n",
    "    .when(F.col('vict_age') < 55, '45-54')\n",
    "    .when(F.col('vict_age') < 65, '55-64')\n",
    "    .when(F.col('vict_age') < 120, '65+ (Senior)')\n",
    "    .otherwise('Invalid')\n",
    ")\n",
    "\n",
    "age_group_dist = df_age_groups.groupBy('age_group').count().orderBy(F.desc('count'))\n",
    "\n",
    "print(\"Crime Victims by Age Group:\")\n",
    "display(age_group_dist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e935b36-e432-4c65-8b1a-20ac0938fbb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Victim Sex Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b8666b1-065b-4b2a-baf8-18991b671aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Victim Sex analysis\n",
    "print(\"VICTIM SEX ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sex_distribution = df_raw.groupBy('vict_sex').count().orderBy(F.desc('count'))\n",
    "\n",
    "print(\"Victim Sex Distribution:\")\n",
    "display(sex_distribution)\n",
    "\n",
    "# Expected values: F, M, X, H, - (dash), null\n",
    "expected_sex_values = ['F', 'M', 'X', 'H', '-', None]\n",
    "sex_values = df_raw.select('vict_sex').distinct().collect()\n",
    "actual_sex_values = [row['vict_sex'] for row in sex_values]\n",
    "\n",
    "print(f\"\\nUnique sex values found: {actual_sex_values}\")\n",
    "print(f\"Expected values: F (Female), M (Male), X (Unknown), H (Non-binary), - (Unknown)\")\n",
    "\n",
    "# Count null vs non-null\n",
    "null_sex = df_raw.filter(F.col('vict_sex').isNull()).count()\n",
    "non_null_sex = df_raw.filter(F.col('vict_sex').isNotNull()).count()\n",
    "\n",
    "print(f\"\\nNull vict_sex: {null_sex:,} ({(null_sex/total_records)*100:.2f}%)\")\n",
    "print(f\"Non-Null vict_sex: {non_null_sex:,} ({(non_null_sex/total_records)*100:.2f}%)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Victim Descent analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"VICTIM DESCENT (ETHNICITY) ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "descent_distribution = df_raw.groupBy('vict_descent').count().orderBy(F.desc('count'))\n",
    "\n",
    "print(\"Victim Descent Distribution:\")\n",
    "display(descent_distribution)\n",
    "\n",
    "# Descent code mapping (from documentation)\n",
    "descent_mapping = {\n",
    "    'A': 'Other Asian',\n",
    "    'B': 'Black',\n",
    "    'C': 'Chinese',\n",
    "    'D': 'Cambodian',\n",
    "    'F': 'Filipino',\n",
    "    'G': 'Guamanian',\n",
    "    'H': 'Hispanic/Latin/Mexican',\n",
    "    'I': 'American Indian/Alaskan Native',\n",
    "    'J': 'Japanese',\n",
    "    'K': 'Korean',\n",
    "    'L': 'Laotian',\n",
    "    'O': 'Other',\n",
    "    'P': 'Pacific Islander',\n",
    "    'S': 'Samoan',\n",
    "    'U': 'Hawaiian',\n",
    "    'V': 'Vietnamese',\n",
    "    'W': 'White',\n",
    "    'X': 'Unknown',\n",
    "    'Z': 'Asian Indian',\n",
    "    '-': 'Unknown'\n",
    "}\n",
    "\n",
    "print(\"\\nDescent Code Reference:\")\n",
    "for code, description in sorted(descent_mapping.items()):\n",
    "    print(f\"  {code}: {description}\")\n",
    "\n",
    "# Check for unexpected codes\n",
    "descent_values = df_raw.select('vict_descent').distinct().collect()\n",
    "actual_descent_codes = [row['vict_descent'] for row in descent_values if row['vict_descent'] is not None]\n",
    "unexpected_codes = [code for code in actual_descent_codes if code not in descent_mapping.keys()]\n",
    "\n",
    "if unexpected_codes:\n",
    "    print(f\"\\n⚠️ WARNING: Found unexpected descent codes: {unexpected_codes}\")\n",
    "else:\n",
    "    print(\"\\n✅ All descent codes are valid\")\n",
    "\n",
    "# Null analysis\n",
    "null_descent = df_raw.filter(F.col('vict_descent').isNull()).count()\n",
    "print(f\"\\nNull vict_descent: {null_descent:,} ({(null_descent/total_records)*100:.2f}%)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Cross-analysis: Age vs Sex vs Descent\n",
    "print(\"=\" * 80)\n",
    "print(\"DEMOGRAPHIC CROSS-ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for records missing all demographic info\n",
    "missing_all_demographics = df_raw.filter(\n",
    "    F.col('vict_age').isNull() | \n",
    "    F.col('vict_sex').isNull() | \n",
    "    F.col('vict_descent').isNull()\n",
    ").count()\n",
    "\n",
    "print(f\"Records missing any demographic info: {missing_all_demographics:,} ({(missing_all_demographics/total_records)*100:.2f}%)\")\n",
    "\n",
    "# Most common demographic profile\n",
    "print(\"\\nTop 10 Victim Demographic Profiles (Age Group + Sex + Descent):\")\n",
    "top_profiles = df_age_groups.groupBy('age_group', 'vict_sex', 'vict_descent') \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc('count')) \\\n",
    "    .limit(10)\n",
    "display(top_profiles)\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-profiling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
